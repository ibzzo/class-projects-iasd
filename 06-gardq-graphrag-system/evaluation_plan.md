# Plan d'√âvaluation Exp√©rimentale pour GARDQ

## üìä Vue d'ensemble

Ce document pr√©sente un plan d'√©valuation complet pour le syst√®me GARDQ, bas√© sur la m√©thodologie du papier LinkedIn (arxiv:2404.17723) et les meilleures pratiques actuelles en √©valuation des syst√®mes RAG.

## üéØ Objectifs de l'√âvaluation

1. **√âvaluer la performance de r√©cup√©ration** : Mesurer la capacit√© du syst√®me √† retrouver les tickets pertinents
2. **√âvaluer la qualit√© de g√©n√©ration** : V√©rifier la pertinence et la qualit√© des solutions sugg√©r√©es
3. **Mesurer l'impact op√©rationnel** : Quantifier la r√©duction du temps de r√©solution
4. **Valider l'approche Knowledge Graph** : Comparer avec des approches baseline

## üìê M√©triques d'√âvaluation

### 1. M√©triques de R√©cup√©ration (Retrieval)

#### a) **MRR (Mean Reciprocal Rank)**
- Mesure la position moyenne du premier r√©sultat pertinent
- Formule : MRR = (1/|Q|) √ó Œ£(1/rank_i)
- Cible : > 0.7 (am√©lioration de 77.6% comme LinkedIn)

#### b) **Recall@K** (K = 1, 3, 5, 10)
- Proportion de tickets pertinents dans les K premiers r√©sultats
- Formule : Recall@K = |relevant ‚à© retrieved_K| / |relevant|
- Cibles :
  - Recall@1 : > 0.5
  - Recall@5 : > 0.8
  - Recall@10 : > 0.9

#### c) **NDCG@K** (Normalized Discounted Cumulative Gain)
- √âvalue l'ordre des r√©sultats en tenant compte de leur pertinence
- Particuli√®rement important pour les suggestions ordonn√©es

### 2. M√©triques de G√©n√©ration

#### a) **BLEU Score**
- Compare les solutions g√©n√©r√©es avec des solutions de r√©f√©rence
- Cible : Am√©lioration de 0.32+ par rapport au baseline

#### b) **ROUGE-L**
- Mesure la plus longue sous-s√©quence commune
- Utile pour √©valuer la coh√©rence structurelle

#### c) **METEOR**
- Prend en compte les synonymes et paraphrases
- Plus adapt√© aux r√©ponses techniques vari√©es

#### d) **BERTScore**
- √âvaluation s√©mantique bas√©e sur les embeddings
- Capture mieux le sens que les m√©triques bas√©es sur les n-grammes

### 3. M√©triques Sp√©cifiques au Domaine

#### a) **Taux de R√©solution Exacte**
- % de cas o√π la solution sugg√©r√©e correspond exactement √† la solution appliqu√©e
- Formule : solutions_exactes / total_tickets

#### b) **Couverture des Entit√©s**
- % d'entit√©s du ticket correctement identifi√©es (Application, Priority, etc.)
- Mesure la compr√©hension du contexte

#### c) **Temps de R√©solution M√©dian**
- R√©duction du temps entre soumission et r√©solution
- Cible : -28.6% comme LinkedIn

#### d) **Taux d'Hallucination**
- % de suggestions contenant des informations non v√©rifiables
- Cible : < 10%

## üß™ Setup Exp√©rimental

### 1. Pr√©paration des Donn√©es

#### Dataset Principal
```python
# Structure du dataset d'√©valuation
{
    "test_set": {
        "size": 1000,  # tickets pour l'√©valuation
        "split": "80/10/10",  # train/val/test
        "stratification": "par type d'incident et priorit√©"
    },
    "annotations": {
        "tickets_similaires": "3-5 par ticket test",
        "solutions_reference": "solutions valid√©es par experts",
        "entites_annotees": "toutes les entit√©s m√©tier"
    }
}
```

#### Cr√©ation du Ground Truth
1. S√©lectionner 1000 tickets r√©solus avec succ√®s
2. Pour chaque ticket :
   - Identifier manuellement 3-5 tickets similaires
   - Valider la solution appliqu√©e
   - Annoter les entit√©s m√©tier

### 2. Baselines de Comparaison

#### a) **BM25 Baseline**
- Recherche textuelle classique sans graphe
- Impl√©mentation avec Elasticsearch ou Lucene

#### b) **Embeddings Purs**
- Recherche vectorielle seule (sans graphe)
- Utiliser le m√™me mod√®le d'embeddings

#### c) **RAG Simple**
- R√©cup√©ration + g√©n√©ration sans structure de graphe
- GPT-4 avec contexte de tickets concat√©n√©s

#### d) **Version simplifi√©e**
- Version sans extraction de sous-graphes
- Recherche par embeddings uniquement

### 3. Protocole d'√âvaluation

#### Phase 1 : √âvaluation de la R√©cup√©ration
```python
def evaluate_retrieval(system, test_queries, ground_truth):
    metrics = {
        'mrr': [],
        'recall@1': [],
        'recall@5': [],
        'recall@10': [],
        'ndcg@5': []
    }
    
    for query in test_queries:
        results = system.search(query, k=10)
        relevant = ground_truth[query.id]
        
        # Calculer les m√©triques
        metrics['mrr'].append(calculate_mrr(results, relevant))
        metrics['recall@1'].append(calculate_recall_at_k(results, relevant, k=1))
        # ...
        
    return aggregate_metrics(metrics)
```

#### Phase 2 : √âvaluation de la G√©n√©ration
```python
def evaluate_generation(system, test_tickets, reference_solutions):
    metrics = {
        'bleu': [],
        'rouge': [],
        'meteor': [],
        'bertscore': []
    }
    
    for ticket in test_tickets:
        generated = system.generate_solution(ticket)
        reference = reference_solutions[ticket.id]
        
        # Calculer les m√©triques
        metrics['bleu'].append(calculate_bleu(generated, reference))
        # ...
        
    return aggregate_metrics(metrics)
```

#### Phase 3 : √âvaluation End-to-End
```python
def evaluate_end_to_end(system, test_scenarios):
    results = {
        'resolution_accuracy': [],
        'entity_coverage': [],
        'hallucination_rate': [],
        'time_to_solution': []
    }
    
    for scenario in test_scenarios:
        start_time = time.time()
        suggestion = system.process_incident(scenario.query)
        
        # √âvaluer la qualit√© compl√®te
        results['resolution_accuracy'].append(
            check_resolution_match(suggestion, scenario.ground_truth)
        )
        # ...
        
    return aggregate_results(results)
```

## üìä Plan d'Impl√©mentation

### Semaine 1-2 : Pr√©paration des Donn√©es
- [ ] S√©lectionner et nettoyer 1000 tickets
- [ ] Cr√©er les annotations manuelles
- [ ] D√©velopper les scripts de pr√©traitement

### Semaine 3 : Impl√©mentation des Baselines
- [ ] BM25 avec Elasticsearch
- [ ] Syst√®me d'embeddings purs
- [ ] RAG simple sans graphe

### Semaine 4 : Framework d'√âvaluation
- [ ] Impl√©menter les m√©triques de r√©cup√©ration
- [ ] Impl√©menter les m√©triques de g√©n√©ration
- [ ] Cr√©er le pipeline d'√©valuation automatis√©

### Semaine 5-6 : Exp√©rimentations
- [ ] √âvaluer tous les syst√®mes
- [ ] Ablation studies sur GARDQ
- [ ] Analyse des erreurs

### Semaine 7 : Analyse et Rapport
- [ ] Analyse statistique des r√©sultats
- [ ] G√©n√©ration des graphiques
- [ ] R√©daction du rapport d'√©valuation

## üî¨ Ablation Studies

### 1. Impact du Knowledge Graph
- Comparer GARDQ avec/sans structure de graphe
- Mesurer l'apport de chaque type de relation

### 2. Taille du Contexte
- Varier k (nombre de tickets r√©cup√©r√©s)
- Identifier le trade-off optimal

### 3. Mod√®les de G√©n√©ration
- Comparer GPT-4, GPT-3.5, Claude
- √âvaluer l'impact sur la qualit√©/co√ªt

### 4. Types d'Embeddings
- Tester diff√©rents mod√®les (multilingual, domain-specific)
- Mesurer l'impact sur la r√©cup√©ration

## üìà R√©sultats Attendus

### Am√©lioration Cibles (vs Baseline BM25)
- MRR : +70-80%
- BLEU : +0.3-0.4
- Temps de r√©solution : -25-30%
- Taux d'hallucination : < 10%

### Livrables
1. **Rapport technique** : M√©thodologie compl√®te et r√©sultats
2. **Dashboard de m√©triques** : Visualisation interactive
3. **Code d'√©valuation** : Framework r√©utilisable
4. **Dataset annot√©** : Pour recherches futures

## üöÄ Prochaines √âtapes

1. **Validation du plan** avec les parties prenantes
2. **Collecte des donn√©es** d'√©valuation
3. **D√©veloppement** du framework d'√©valuation
4. **Ex√©cution** des exp√©riences
5. **Publication** des r√©sultats

Ce plan d'√©valuation permet une validation rigoureuse du syst√®me GARDQ tout en fournissant des insights actionnables pour son am√©lioration continue.