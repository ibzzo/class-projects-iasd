# Pr√©diction de S√©ries Temporelles Financi√®res
## Comp√©tition Kaggle - Pr√©sentation des R√©sultats

---

## üìä Vue d'ensemble du Projet

### Objectif
Pr√©dire des valeurs financi√®res futures bas√©es sur des donn√©es historiques de s√©ries temporelles avec 133 features num√©riques.

### Donn√©es
- **P√©riode d'entra√Ænement** : 2008-11-25 √† 2019-09-03 (2,811 observations)
- **P√©riode de test** : 2019-09-04 √† 2024-04-17 (1,206 observations)
- **Nombre de features initiales** : 133
- **Variable cible** : `ToPredict` (valeurs entre -0.626 et 1.145)

---

## üî¨ M√©thodologie

### 1. Exploration et Analyse des Donn√©es

#### Statistiques de la variable cible
- **Moyenne** : 0.136481
- **√âcart-type** : 0.278196
- **Min/Max** : -0.626089 / 1.144987

#### D√©couverte cl√©
- **Features_38** domine compl√®tement avec un score d'importance de 1.0
- D√©cision strat√©gique : Exclure cette feature pour √©viter l'overfitting

### 2. Feature Engineering Avanc√©

#### A. Features Temporelles
- Extraction de composants : ann√©e, mois, jour, jour de la semaine, trimestre
- Features cycliques : sin/cos pour capturer la p√©riodicit√©
- Indicateurs bool√©ens : d√©but/fin de mois, trimestre, ann√©e

#### B. Features Statistiques Mobiles
- **Rolling statistics** sur fen√™tres [5, 7, 14, 21, 30, 60] jours :
  - Moyenne, √©cart-type, min, max
  - Skewness pour les fen√™tres plus grandes
  - Ratios avec la moyenne mobile

#### C. Features de D√©calage (Lag)
- Lags multiples : [1, 2, 3, 5, 7, 14, 21, 30] jours
- Diff√©rences et ratios avec les valeurs pass√©es
- Pourcentage de changement

#### D. Features d'Interaction
- Produits, divisions, sommes et diff√©rences entre top features
- Features polynomiales (carr√©, cube, racine carr√©e) pour les 3 meilleures

#### E. S√©lection des Features
- Combinaison de 3 m√©thodes :
  - F-statistics (20%)
  - Mutual Information (20%)
  - LightGBM importance (60%)
- S√©lection finale : 40-45 features les plus importantes

### 3. Strat√©gie de Mod√©lisation

#### Validation Temporelle Stricte
- **TimeSeriesSplit** avec 7-8 folds
- Respect de l'ordre chronologique
- Pas de data leakage

#### Ensemble de Mod√®les
1. **LightGBM** (2 versions avec param√®tres diff√©rents)
2. **XGBoost**
3. **CatBoost**
4. **Random Forest**
5. **Gradient Boosting**
6. **Extra Trees**

### 4. Optimisation des Hyperparam√®tres

#### Version Parameter Tuning (Meilleure performance)
```python
LightGBM:
- n_estimators: 1500
- learning_rate: 0.008
- num_leaves: 40
- max_depth: 6
- R√©gularisation: reg_alpha=0.05, reg_lambda=0.05
```

#### Version Fine-Tuned
```python
LightGBM principal:
- n_estimators: 2000
- learning_rate: 0.005
- num_leaves: 45
- max_depth: 7
- Early stopping: 150 rounds
```

---

## üìà R√©sultats

### Performance des Mod√®les (Cross-Validation)

#### Version Parameter Tuning
| Mod√®le | RMSE Moyen | √âcart-type | Poids Final |
|--------|------------|------------|-------------|
| LightGBM | 0.1244 | ¬±0.0193 | **98.6%** |
| XGBoost | 0.1266 | ¬±0.0204 | 0.0% |
| CatBoost | 0.1314 | ¬±0.0229 | 1.4% |
| Random Forest | 0.1359 | ¬±0.0256 | 0.0% |
| Extra Trees | 0.1573 | ¬±0.0254 | 0.0% |
| Gradient Boosting | 0.1341 | ¬±0.0231 | 0.0% |

**Score Ensemble : 0.1618 RMSE**

#### Version Fine-Tuned
| Mod√®le | RMSE Moyen | √âcart-type | Poids Final |
|--------|------------|------------|-------------|
| LightGBM | 0.1068 | ¬±0.0109 | 43.1% |
| XGBoost | 0.1070 | ¬±0.0106 | **56.2%** |
| CatBoost | 0.1222 | ¬±0.0332 | 0.0% |
| LightGBM v2 | 0.1095 | ¬±0.0185 | 0.7% |
| Random Forest | 0.1189 | ¬±0.0226 | 0.0% |
| Gradient Boosting | 0.1217 | ¬±0.0237 | 0.0% |

**Score Ensemble : 0.1349 RMSE**

### Features les Plus Importantes

#### Top 10 Features (Version Parameter Tuning)
1. Features_101 (3.07%)
2. Features_41 (3.06%)
3. Features_38 (3.05%) - Gard√©e malgr√© sa dominance
4. Features_44 (2.49%)
5. Features_104 (2.38%)
6. Features_101_lag_1 (2.31%)
7. Features_101_roll_min_7 (2.14%)
8. Features_85_roll_ratio_30 (1.84%)
9. Features_101_roll_mean_7 (1.71%)
10. Features_41_roll_min_7 (1.68%)

### Post-Processing

1. **Correction des valeurs n√©gatives** si non pr√©sentes dans le train
2. **Clipping des valeurs extr√™mes** (percentiles 0.5-99.5)
3. **Ajustement de la variance** si ratio > 2 ou < 0.5
4. **Lissage temporel adaptatif** pour s√©ries > 100 points

---

## üéØ Points Cl√©s du Succ√®s

### 1. Gestion de la Feature Dominante
- Identification pr√©coce de Features_38
- Strat√©gie d'exclusion/r√©duction d'impact

### 2. Feature Engineering Robuste
- Plus de 600-1100 features cr√©√©es
- S√©lection rigoureuse des meilleures

### 3. Validation Temporelle
- Respect strict de l'ordre chronologique
- Pas de fuite d'information future

### 4. Ensemble Optimis√©
- LightGBM domine mais reste performant
- Poids optimis√©s par minimisation RMSE

### 5. Post-Processing Conservateur
- Ajustements minimaux
- Pr√©servation de la distribution originale

---

## üí° Insights et Apprentissages

### Ce qui a fonctionn√©
1. **Features temporelles multiples** : Capturer diff√©rentes p√©riodicit√©s
2. **Rolling statistics √©tendues** : Fen√™tres multiples pour diff√©rents horizons
3. **LightGBM avec r√©gularisation** : Meilleur √©quilibre biais-variance
4. **Validation temporelle stricte** : √âvaluation r√©aliste

### D√©fis rencontr√©s
1. **Feature dominante** : N√©cessit√© de g√©rer Features_38
2. **Overfitting potentiel** : Avec >1000 features cr√©√©es
3. **D√©s√©quilibre des mod√®les** : LightGBM surpasse largement les autres

### Am√©liorations possibles
1. **Feature engineering cibl√©** : Bas√© sur la connaissance du domaine
2. **Mod√®les sp√©cialis√©s** : LSTM, Prophet pour s√©ries temporelles
3. **Stacking plus sophistiqu√©** : Meta-learner au lieu de poids fixes
4. **Analyse des r√©sidus** : Pour identifier les patterns manqu√©s

---

## üìä Visualisations Cl√©s

### 1. Distribution des Pr√©dictions
- Bonne correspondance avec la distribution du train
- L√©g√®re sous-estimation de la variance (intentionnelle)

### 2. √âvolution Temporelle
- Capture des tendances principales
- Lissage appropri√© du bruit

### 3. Importance des Features
- Dominance des features originales (101, 41, 38, 44)
- Importance significative des lag_1 et rolling_mean_7

### 4. Performance par Fold
- Stabilit√© relative √† travers les folds
- Fold 6 syst√©matiquement plus difficile (changement de r√©gime?)

---

## üèÜ Conclusion

### R√©sultats Finaux
- **Meilleur score obtenu** : Version Parameter Tuning
- **Approche gagnante** : Feature engineering extensif + LightGBM optimis√©
- **Le√ßon principale** : Parfois, laisser dominer le meilleur mod√®le est optimal

### Valeur Ajout√©e
1. Pipeline reproductible et modulaire
2. Validation rigoureuse sans data leakage
3. Post-processing adaptatif bas√© sur les donn√©es
4. Documentation compl√®te des d√©cisions

### Code et Reproductibilit√©
- Scripts Python modulaires et comment√©s
- Gestion propre des d√©pendances
- Visualisations automatiques pour l'analyse

---

*Projet r√©alis√© dans le cadre d'une comp√©tition Kaggle de pr√©diction de s√©ries temporelles financi√®res*