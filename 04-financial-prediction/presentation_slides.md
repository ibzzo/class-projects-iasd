# PrÃ©diction de SÃ©ries Temporelles FinanciÃ¨res
### CompÃ©tition Kaggle - PrÃ©sentation des RÃ©sultats

---

## Slide 1: Introduction

### ğŸ“Š Contexte du Projet
- **Objectif** : PrÃ©dire des valeurs financiÃ¨res futures
- **Type** : SÃ©ries temporelles avec 133 features
- **PÃ©riode** : 2008-2024 (15+ annÃ©es de donnÃ©es)
- **Challenge** : Feature dominante Ã  gÃ©rer

---

## Slide 2: Les DonnÃ©es

### ğŸ“ˆ AperÃ§u du Dataset

| CaractÃ©ristique | Train | Test |
|-----------------|-------|------|
| Observations | 2,811 | 1,206 |
| PÃ©riode | 2008-2019 | 2019-2024 |
| Features | 133 | 133 |
| Target (moy.) | 0.136 | Ã€ prÃ©dire |

**DÃ©couverte clÃ©** : Features_38 avec score d'importance = 1.0

---

## Slide 3: MÃ©thodologie - Feature Engineering

### ğŸ”§ CrÃ©ation de Features

1. **Temporelles** (20 features)
   - Date, cycliques (sin/cos), indicateurs

2. **Rolling Statistics** (300+ features)
   - Windows: 5, 7, 14, 21, 30, 60 jours
   - Mean, std, min, max, skew

3. **Lag Features** (200+ features)
   - Lags: 1, 2, 3, 5, 7, 14, 21, 30
   - DiffÃ©rences, ratios, % change

4. **Interactions** (100+ features)
   - Top features: Ã—, Ã·, +, âˆ’
   - Polynomiales: xÂ², xÂ³, âˆšx

**Total : 600-1100 features crÃ©Ã©es**

---

## Slide 4: SÃ©lection des Features

### ğŸ¯ Approche Multi-CritÃ¨res

```
Score Final = 0.2Ã—F-stat + 0.2Ã—MI + 0.6Ã—LightGBM
```

**Top 5 Features SÃ©lectionnÃ©es** :
1. Features_101 (54.3%)
2. Features_41 (45.0%)
3. Features_104 (40.8%)
4. Features_44 (37.1%)
5. Features_50 (27.5%)

*Note: Features_38 exclue pour Ã©viter l'overfitting*

---

## Slide 5: StratÃ©gie de ModÃ©lisation

### ğŸ¤– Ensemble de 6 ModÃ¨les

| ModÃ¨le | HyperparamÃ¨tres ClÃ©s |
|--------|---------------------|
| **LightGBM** | lr=0.008, leaves=40, depth=6 |
| **XGBoost** | lr=0.008, depth=6, Î³=0.01 |
| **CatBoost** | lr=0.025, depth=7, l2=2 |
| **Random Forest** | trees=500, depth=10 |
| **Extra Trees** | trees=500, depth=10 |
| **Gradient Boost** | lr=0.008, depth=6 |

**Validation** : TimeSeriesSplit (7 folds)

---

## Slide 6: RÃ©sultats - Performance

### ğŸ“Š Scores de Validation CroisÃ©e

```
LightGBM    : 0.1244 Â± 0.0193 â­
XGBoost     : 0.1266 Â± 0.0204
CatBoost    : 0.1314 Â± 0.0229
RF          : 0.1359 Â± 0.0256
GB          : 0.1341 Â± 0.0231
Extra Trees : 0.1573 Â± 0.0254
```

**Poids Optimaux** :
- LightGBM : 98.6%
- CatBoost : 1.4%

---

## Slide 7: Post-Processing

### ğŸ¨ Ajustements Finaux

1. **Valeurs nÃ©gatives** â†’ Correction si absent du train
2. **Outliers** â†’ Clipping (percentiles 0.5-99.5)
3. **Variance** â†’ Ajustement si ratio â‰  [0.5, 2.0]
4. **Lissage** â†’ Moving average (window=3)

**Impact** :
- Stabilisation des prÃ©dictions
- Meilleure gÃ©nÃ©ralisation
- RÃ©duction du bruit

---

## Slide 8: Features Importantes

### ğŸ” Top 10 Contributeurs

| Rank | Feature | Type | Importance |
|------|---------|------|------------|
| 1 | Features_101 | Original | 3.07% |
| 2 | Features_41 | Original | 3.06% |
| 3 | Features_101_lag_1 | Lag | 2.31% |
| 4 | Features_101_roll_min_7 | Rolling | 2.14% |
| 5 | Features_44 | Original | 2.49% |

**Insights** :
- Features originales dominent
- Lag_1 trÃ¨s informatif
- Rolling sur 7 jours optimal

---

## Slide 9: Visualisations ClÃ©s

### ğŸ“ˆ Analyses Graphiques

1. **Distribution** : Match train/test âœ“
2. **Ã‰volution temporelle** : Tendances capturÃ©es âœ“
3. **Q-Q Plot** : NormalitÃ© acceptable âœ“
4. **StabilitÃ©** : RMSE constant sur les folds âœ“

![Performance par Fold]
- Fold 1-5: Stable (~0.12)
- Fold 6: Pic (0.16) - changement de rÃ©gime?
- Fold 7: Retour normal (0.11)

---

## Slide 10: Version Fine-Tuned

### ğŸš€ AmÃ©liorations AvancÃ©es

**Changements** :
- 45 features (vs 40)
- 8 splits (vs 7)
- Plus de modÃ¨les (LightGBM v2)
- Features polynomiales

**RÃ©sultats** :
- Meilleur Ã©quilibre : LGB 43%, XGB 56%
- RMSE amÃ©liorÃ© sur CV
- Plus robuste mais score Kaggle lÃ©gÃ¨rement infÃ©rieur

---

## Slide 11: LeÃ§ons Apprises

### ğŸ’¡ Insights ClÃ©s

**âœ… SuccÃ¨s** :
1. Feature engineering extensif payant
2. LightGBM excellent pour sÃ©ries temporelles
3. Validation temporelle stricte essentielle
4. Gestion proactive des features dominantes

**âš ï¸ DÃ©fis** :
1. Ã‰quilibrer complexitÃ© vs overfitting
2. Features_38 : blessing ou curse?
3. Ensemble dÃ©sÃ©quilibrÃ© mais performant

---

## Slide 12: Conclusion

### ğŸ† RÃ©sultats Finaux

- **Meilleur Score** : Version Parameter Tuning
- **RMSE Final** : 0.1618
- **Approche Gagnante** : 
  - Feature engineering massif
  - LightGBM dominant (98.6%)
  - Post-processing minimal

### ğŸ“š Contributions
- Pipeline ML complet et reproductible
- Gestion des sÃ©ries temporelles sans data leakage
- Documentation dÃ©taillÃ©e des dÃ©cisions

---

## Slide 13: Prochaines Ã‰tapes

### ğŸ”® AmÃ©liorations Futures

1. **ModÃ¨les SpÃ©cialisÃ©s**
   - LSTM/GRU pour patterns temporels
   - Prophet pour saisonnalitÃ©

2. **Feature Engineering**
   - Connaissance domaine finance
   - Indicateurs techniques

3. **Ensemble AvancÃ©**
   - Stacking avec meta-learner
   - Blending dynamique

4. **Analyse Approfondie**
   - Ã‰tude des rÃ©sidus
   - DÃ©tection d'anomalies

---

## Questions?

### ğŸ“§ Contact
- Code disponible sur GitHub
- Scripts Python modulaires
- Documentation complÃ¨te

**Merci de votre attention!**

---

## Annexe: Commandes ClÃ©s

### ExÃ©cution du Meilleur ModÃ¨le

```bash
# Activation environnement
source venv_kaggle/bin/activate

# Lancer le modÃ¨le gagnant
python financial_prediction_parameter_tuning.py

# Output:
# âœ… submission_parameter_tuning.csv
# ğŸ“Š predictions_parameter_tuning_analysis.png
```

### Structure des Fichiers
```
kaggle_2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ financial_prediction_parameter_tuning.py â­
â”œâ”€â”€ submission_parameter_tuning.csv
â””â”€â”€ predictions_parameter_tuning_analysis.png
```