#!/usr/bin/env python3
"""
Script complet pour prÃ©parer les donnÃ©es de l'expÃ©rience "Mission: Impossible Language Models"
Version franÃ§aise avec donnÃ©es FQuAD

Auteur: Assistant Claude
Date: 2024
"""

import json
import random
import os
import re
import sys
from typing import List, Dict, Any
from collections import Counter

# ============================================================================
# SECTION 1: INSTALLATION DES DÃ‰PENDANCES
# ============================================================================

print("=" * 80)
print("MISSION: IMPOSSIBLE LANGUAGE MODELS - PRÃ‰PARATION DES DONNÃ‰ES")
print("=" * 80)

# VÃ©rifier et installer les dÃ©pendances
print("\nğŸ“¦ VÃ©rification des dÃ©pendances...")

try:
    from datasets import load_dataset
    print("âœ“ datasets installÃ©")
except ImportError:
    print("Installation de datasets...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "datasets"])
    from datasets import load_dataset

try:
    import spacy
    print("âœ“ spaCy installÃ©")
except ImportError:
    print("Installation de spaCy...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "spacy"])
    import spacy

# TÃ©lÃ©charger le modÃ¨le franÃ§ais si nÃ©cessaire
try:
    nlp = spacy.load("fr_core_news_sm")
    print("âœ“ ModÃ¨le franÃ§ais spaCy chargÃ©")
except OSError:
    print("TÃ©lÃ©chargement du modÃ¨le franÃ§ais...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "spacy", "download", "fr_core_news_sm"])
    nlp = spacy.load("fr_core_news_sm")

# Configuration
random.seed(42)

# CrÃ©er la structure de dossiers
os.makedirs('synthetic_languages/shuffle', exist_ok=True)
os.makedirs('synthetic_languages/reverse', exist_ok=True)
os.makedirs('synthetic_languages/hop', exist_ok=True)

# ============================================================================
# SECTION 2: EXTRACTION DES DONNÃ‰ES DEPUIS FQUAD
# ============================================================================

print("\n" + "=" * 80)
print("Ã‰TAPE 1: EXTRACTION DES DONNÃ‰ES")
print("=" * 80)

# Charger le dataset FQuAD
print("\nğŸ“š Chargement du dataset FQuAD...")
ds = load_dataset("manu/fquad2_test")

# Extraire tous les contextes uniques
contexts = set()
for split_name in ds.keys():
    print(f"  - Traitement du split '{split_name}': {len(ds[split_name])} exemples")
    for item in ds[split_name]:
        if 'context' in item:
            contexts.add(item['context'])

contexts_list = list(contexts)
print(f"\nâœ“ {len(contexts_list)} contextes uniques extraits")

# ============================================================================
# SECTION 3: DIVISION DES CONTEXTES EN PHRASES
# ============================================================================

def split_into_sentences(text: str) -> List[str]:
    """
    Divise un texte en phrases en gÃ©rant les abrÃ©viations franÃ§aises
    Filtre les phrases entre 10 et 50 mots
    """
    # GÃ©rer les abrÃ©viations courantes pour Ã©viter les fausses coupures
    abbreviations = {
        'M.': 'M@', 'Mme.': 'Mme@', 'Dr.': 'Dr@', 'Prof.': 'Prof@',
        'etc.': 'etc@', 'cf.': 'cf@', 'p.': 'p@', 'ex.': 'ex@',
        'c.-Ã -d.': 'c@-Ã -d@', 'av.': 'av@', 'apr.': 'apr@', 'J.-C.': 'J@-C@',
    }
    
    # Remplacer temporairement les abrÃ©viations
    for abbr, replacement in abbreviations.items():
        text = text.replace(abbr, replacement)
    
    # Diviser par les ponctuations de fin de phrase
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÃ€-Å¸])', text)
    
    # Restaurer les abrÃ©viations
    for abbr, replacement in abbreviations.items():
        sentences = [s.replace(replacement, abbr) for s in sentences]
    
    # Nettoyer et filtrer par longueur
    sentences = [s.strip() for s in sentences if s.strip()]
    sentences = [s for s in sentences if 10 <= len(s.split()) <= 50]
    
    return sentences

print("\nğŸ“ Division des contextes en phrases...")
all_sentences = []
for i, context in enumerate(contexts_list):
    if i % 100 == 0:
        print(f"  Progression: {i}/{len(contexts_list)}")
    sentences = split_into_sentences(context)
    all_sentences.extend(sentences)

# DÃ©dupliquer les phrases
unique_sentences = list(set(all_sentences))
print(f"\nâœ“ {len(unique_sentences)} phrases uniques extraites")

# CrÃ©er les splits train/validation/test
random.shuffle(unique_sentences)
total = len(unique_sentences)
train_size = int(0.8 * total)  # 80%
val_size = int(0.1 * total)    # 10%

train_sentences = unique_sentences[:train_size]
val_sentences = unique_sentences[train_size:train_size + val_size]
test_sentences = unique_sentences[train_size + val_size:]

print(f"\nğŸ“Š Splits crÃ©Ã©s:")
print(f"  - Train: {len(train_sentences)} phrases")
print(f"  - Validation: {len(val_sentences)} phrases")
print(f"  - Test: {len(test_sentences)} phrases")

# ============================================================================
# SECTION 4: ANNOTATION POS AVEC SPACY
# ============================================================================

print("\n" + "=" * 80)
print("Ã‰TAPE 2: ANNOTATION POS")
print("=" * 80)

def annotate_sentence(sentence: str, nlp) -> Dict:
    """
    Annote une phrase avec les informations POS (Part-of-Speech)
    NÃ©cessaire pour les transformations *Hop
    """
    doc = nlp(sentence)
    
    tokens = []
    for token in doc:
        token_info = {
            "text": token.text,
            "lemma": token.lemma_,
            "pos": token.pos_,        # POS universel
            "tag": token.tag_,        # POS dÃ©taillÃ©
            "dep": token.dep_,        # DÃ©pendance syntaxique
            "morph": str(token.morph), # Traits morphologiques
            "is_alpha": token.is_alpha,
            "is_stop": token.is_stop,
            "is_punct": token.is_punct
        }
        
        # Informations spÃ©cifiques pour les verbes (pour *Hop)
        if token.pos_ == "VERB":
            morph_dict = token.morph.to_dict()
            token_info["verb_form"] = morph_dict.get("VerbForm", "")
            token_info["tense"] = morph_dict.get("Tense", "")
            token_info["person"] = morph_dict.get("Person", "")
            token_info["number"] = morph_dict.get("Number", "")
        
        tokens.append(token_info)
    
    return {
        "original": sentence,
        "tokens": tokens,
        "num_tokens": len(tokens),
        "num_words": len([t for t in tokens if t["is_alpha"]]),
        "has_verb": any(t["pos"] == "VERB" for t in tokens)
    }

print("\nğŸ·ï¸  Annotation des phrases avec spaCy...")
tagged_data = {
    "metadata": {
        "source": "FQuAD contexts",
        "total_unique_sentences": len(unique_sentences),
        "language": "fr",
        "tagging_model": "fr_core_news_sm",
        "description": "Phrases annotÃ©es pour l'expÃ©rience Mission: Impossible Language Models"
    },
    "train": [],
    "validation": [],
    "test": []
}

# Annoter le train set
print("\n  Annotation du train set...")
for i, sentence in enumerate(train_sentences):
    if i % 200 == 0:
        print(f"    Progression: {i}/{len(train_sentences)}")
    tagged_data["train"].append(annotate_sentence(sentence, nlp))

# Annoter le validation set
print("\n  Annotation du validation set...")
for i, sentence in enumerate(val_sentences):
    if i % 50 == 0:
        print(f"    Progression: {i}/{len(val_sentences)}")
    tagged_data["validation"].append(annotate_sentence(sentence, nlp))

# Annoter le test set
print("\n  Annotation du test set...")
for i, sentence in enumerate(test_sentences):
    if i % 50 == 0:
        print(f"    Progression: {i}/{len(test_sentences)}")
    tagged_data["test"].append(annotate_sentence(sentence, nlp))

# Sauvegarder les donnÃ©es annotÃ©es
with open('tagged_sentences.json', 'w', encoding='utf-8') as f:
    json.dump(tagged_data, f, ensure_ascii=False, indent=2)

print("\nâœ“ DonnÃ©es annotÃ©es sauvegardÃ©es dans 'tagged_sentences.json'")

# Statistiques sur les POS
verb_count = sum(1 for sent in tagged_data["train"] if sent["has_verb"])
print(f"\nğŸ“Š Statistiques:")
print(f"  - Phrases avec verbes dans le train: {verb_count}/{len(tagged_data['train'])} ({verb_count/len(tagged_data['train'])*100:.1f}%)")

# ============================================================================
# SECTION 5: DÃ‰FINITION DES TRANSFORMATIONS
# ============================================================================

print("\n" + "=" * 80)
print("Ã‰TAPE 3: CRÃ‰ATION DES LANGUES SYNTHÃ‰TIQUES")
print("=" * 80)

# --------------- Transformations *SHUFFLE ---------------

def no_shuffle(sentence_data: Dict) -> str:
    """NoShuffle: Retourne la phrase originale (contrÃ´le)"""
    return sentence_data['original']

def nondeterministic_shuffle(sentence_data: Dict) -> str:
    """NondeterministicShuffle: MÃ©lange alÃ©atoire complet des mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    random.shuffle(tokens)
    return ' '.join(tokens)

def deterministic_shuffle(sentence_data: Dict) -> str:
    """DeterministicShuffle: MÃ©lange dÃ©terministe basÃ© sur la longueur"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    # Utiliser la longueur comme seed pour un mÃ©lange reproductible
    rng = random.Random(len(tokens))
    shuffled = tokens.copy()
    rng.shuffle(shuffled)
    return ' '.join(shuffled)

def local_shuffle(sentence_data: Dict) -> str:
    """LocalShuffle: MÃ©lange par petits blocs de 2-3 mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    result = []
    i = 0
    
    while i < len(tokens):
        # Taille du bloc alÃ©atoire (2 ou 3 mots)
        block_size = random.choice([2, 3])
        block = tokens[i:i+block_size]
        # MÃ©langer le bloc
        random.shuffle(block)
        result.extend(block)
        i += block_size
    
    return ' '.join(result)

def even_odd_shuffle(sentence_data: Dict) -> str:
    """EvenOddShuffle: SÃ©parer positions paires/impaires puis concatÃ©ner"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    even_tokens = [tokens[i] for i in range(0, len(tokens), 2)]
    odd_tokens = [tokens[i] for i in range(1, len(tokens), 2)]
    return ' '.join(even_tokens + odd_tokens)

# --------------- Transformations *REVERSE ---------------

def no_reverse(sentence_data: Dict) -> str:
    """NoReverse: Retourne la phrase originale (contrÃ´le)"""
    return sentence_data['original']

def partial_reverse(sentence_data: Dict) -> str:
    """PartialReverse: Inverser une partie aprÃ¨s un marqueur R placÃ© alÃ©atoirement"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    
    if len(tokens) < 3:
        return ' '.join(tokens)
    
    # Placer le marqueur R Ã  une position alÃ©atoire
    r_position = random.randint(1, len(tokens) - 2)
    
    # Construire: dÃ©but + R + fin inversÃ©e
    result = tokens[:r_position] + ['R'] + tokens[r_position:][::-1]
    return ' '.join(result)

def full_reverse(sentence_data: Dict) -> str:
    """FullReverse: Inverser complÃ¨tement l'ordre des mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    return ' '.join(tokens[::-1])

# --------------- Transformations *HOP ---------------

def get_verb_indices(sentence_data: Dict) -> List[int]:
    """Trouve tous les indices des verbes dans la phrase"""
    return [i for i, token in enumerate(sentence_data['tokens']) 
            if token['pos'] == 'VERB']

def inflect_verb(token: Dict, should_pluralize: bool) -> str:
    """
    Inflexion simple du verbe franÃ§ais
    Note: ImplÃ©mentation basique, une vraie conjugaison nÃ©cessiterait plus de rÃ¨gles
    """
    text = token['text']
    
    # Si on doit mettre au pluriel un verbe singulier
    if should_pluralize and token.get('number') == 'Sing':
        if text.endswith('e'):
            return text + 'nt'
        elif text.endswith('t'):
            return text[:-1] + 'ent'
        else:
            return text + 'ent'
    # Si on doit mettre au singulier un verbe pluriel
    elif not should_pluralize and token.get('number') == 'Plur':
        if text.endswith('ent'):
            return text[:-3]
        elif text.endswith('nt'):
            return text[:-2]
    
    return text

def no_hop(sentence_data: Dict) -> str:
    """NoHop: Ajouter des marqueurs S/P mais sans transformation du verbe"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    # Choisir un verbe alÃ©atoire
    verb_idx = random.choice(verb_indices)
    
    # Placer un marqueur S (singulier) ou P (pluriel)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(1, min(4, verb_idx)) if verb_idx > 0 else 0)
    
    result = tokens[:marker_position] + [marker] + tokens[marker_position:]
    return ' '.join(result)

def token_hop(sentence_data: Dict) -> str:
    """TokenHop: Inflexion du verbe basÃ©e sur le comptage de tokens aprÃ¨s le marqueur"""
    tokens_data = sentence_data['tokens']
    tokens = [t['text'] for t in tokens_data]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    verb_idx = random.choice(verb_indices)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(5, min(8, verb_idx)) if verb_idx > 5 else 0)
    
    # Compter 4 tokens aprÃ¨s le marqueur
    count_position = marker_position + 4
    
    # RÃ¨gle impossible: inflexion basÃ©e sur la paritÃ© de la position
    should_pluralize = (count_position % 2 == 0) if marker == 'S' else (count_position % 2 == 1)
    
    # Construire la phrase avec verbe modifiÃ©
    result = tokens[:marker_position] + [marker] + tokens[marker_position:verb_idx]
    result.append(inflect_verb(tokens_data[verb_idx], should_pluralize))
    result.extend(tokens[verb_idx + 1:])
    
    return ' '.join(result)

def word_hop(sentence_data: Dict) -> str:
    """WordHop: Inflexion du verbe basÃ©e sur le comptage de mots (non-ponctuation)"""
    tokens_data = sentence_data['tokens']
    tokens = [t['text'] for t in tokens_data]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    verb_idx = random.choice(verb_indices)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(5, min(8, verb_idx)) if verb_idx > 5 else 0)
    
    # Compter 4 mots (pas la ponctuation) aprÃ¨s le marqueur
    word_count = 0
    for i in range(marker_position, len(tokens_data)):
        if tokens_data[i]['is_alpha']:
            word_count += 1
            if word_count == 4:
                break
    
    # RÃ¨gle impossible: inflexion basÃ©e sur si on a atteint 4 mots
    should_pluralize = (word_count == 4) if marker == 'S' else (word_count != 4)
    
    # Construire la phrase
    result = tokens[:marker_position] + [marker] + tokens[marker_position:verb_idx]
    result.append(inflect_verb(tokens_data[verb_idx], should_pluralize))
    result.extend(tokens[verb_idx + 1:])
    
    return ' '.join(result)

# ============================================================================
# SECTION 6: APPLICATION DES TRANSFORMATIONS
# ============================================================================

# Dictionnaire de toutes les transformations
transformations = {
    # *Shuffle (5 variantes)
    'NoShuffle': no_shuffle,
    'NondeterministicShuffle': nondeterministic_shuffle,
    'DeterministicShuffle': deterministic_shuffle,
    'LocalShuffle': local_shuffle,
    'EvenOddShuffle': even_odd_shuffle,
    # *Reverse (3 variantes)
    'NoReverse': no_reverse,
    'PartialReverse': partial_reverse,
    'FullReverse': full_reverse,
    # *Hop (3 variantes)
    'NoHop': no_hop,
    'TokenHop': token_hop,
    'WordHop': word_hop
}

print("\nğŸ”„ Application des 11 transformations...")

# Appliquer chaque transformation
for trans_name, trans_func in transformations.items():
    print(f"\n  Transformation: {trans_name}")
    
    # CrÃ©er le dataset transformÃ©
    transformed_data = {
        "metadata": {
            **tagged_data["metadata"],
            "transformation": trans_name,
            "transformation_type": "shuffle" if "Shuffle" in trans_name else "reverse" if "Reverse" in trans_name else "hop"
        }
    }
    
    # Transformer chaque split
    for split in ["train", "validation", "test"]:
        transformed_data[split] = []
        
        # RÃ©initialiser la seed pour la reproductibilitÃ©
        random.seed(42)
        
        errors = 0
        for sentence_data in tagged_data[split]:
            try:
                transformed_text = trans_func(sentence_data)
                transformed_data[split].append({
                    "original": sentence_data["original"],
                    "transformed": transformed_text,
                    "num_tokens": sentence_data["num_tokens"],
                    "has_verb": sentence_data["has_verb"]
                })
            except Exception as e:
                # En cas d'erreur, utiliser l'original
                errors += 1
                transformed_data[split].append({
                    "original": sentence_data["original"],
                    "transformed": sentence_data["original"],
                    "num_tokens": sentence_data["num_tokens"],
                    "has_verb": sentence_data["has_verb"]
                })
        
        if errors > 0:
            print(f"    âš ï¸  {errors} erreurs dans le split {split}")
    
    # DÃ©terminer le sous-dossier
    if trans_name.endswith("Shuffle"):
        subdir = "shuffle"
    elif trans_name.endswith("Reverse"):
        subdir = "reverse"
    else:
        subdir = "hop"
    
    # Sauvegarder
    output_path = f"synthetic_languages/{subdir}/{trans_name}.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(transformed_data, f, ensure_ascii=False, indent=2)
    
    print(f"    âœ“ SauvegardÃ© dans {output_path}")
    
    # Afficher un exemple
    example = transformed_data["train"][0]
    print(f"    Exemple: {example['original'][:50]}...")
    print(f"    â†’ {example['transformed'][:50]}...")

# ============================================================================
# SECTION 7: RÃ‰SUMÃ‰ FINAL
# ============================================================================

print("\n" + "=" * 80)
print("RÃ‰SUMÃ‰")
print("=" * 80)

print("\nâœ… PrÃ©paration des donnÃ©es terminÃ©e avec succÃ¨s!")
print(f"\nğŸ“Š Statistiques finales:")
print(f"  - Contextes FQuAD traitÃ©s: {len(contexts_list)}")
print(f"  - Phrases uniques extraites: {len(unique_sentences)}")
print(f"  - RÃ©partition: {len(train_sentences)} train / {len(val_sentences)} val / {len(test_sentences)} test")
print(f"  - Transformations appliquÃ©es: 11")

print("\nğŸ“ Fichiers crÃ©Ã©s:")
print("  - tagged_sentences.json (donnÃ©es annotÃ©es)")
print("  - synthetic_languages/")
print("    â”œâ”€â”€ shuffle/ (5 fichiers)")
print("    â”œâ”€â”€ reverse/ (3 fichiers)")
print("    â””â”€â”€ hop/ (3 fichiers)")

print("\nğŸ¯ Prochaine Ã©tape:")
print("  Utiliser ces donnÃ©es pour entraÃ®ner des modÃ¨les de langage")
print("  et comparer les performances entre langues naturelles et impossibles")

print("\n" + "=" * 80)
print("FIN DU SCRIPT")
print("=" * 80)