#!/usr/bin/env python3
"""
Script complet pour pr√©parer les donn√©es de l'exp√©rience "Mission: Impossible Language Models"
Version fran√ßaise avec donn√©es FQuAD

Auteur: Assistant Claude
Date: 2024
"""

import json
import random
import os
import re
import sys
from typing import List, Dict, Any
from collections import Counter

# ============================================================================
# SECTION 1: INSTALLATION DES D√âPENDANCES
# ============================================================================

print("=" * 80)
print("MISSION: IMPOSSIBLE LANGUAGE MODELS - PR√âPARATION DES DONN√âES")
print("=" * 80)

# V√©rifier et installer les d√©pendances
print("\nüì¶ V√©rification des d√©pendances...")

try:
    from datasets import load_dataset
    print("‚úì datasets install√©")
except ImportError:
    print("Installation de datasets...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "datasets"])
    from datasets import load_dataset

try:
    import spacy
    print("‚úì spaCy install√©")
except ImportError:
    print("Installation de spaCy...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "spacy"])
    import spacy

# T√©l√©charger le mod√®le fran√ßais si n√©cessaire
try:
    nlp = spacy.load("fr_core_news_sm")
    print("‚úì Mod√®le fran√ßais spaCy charg√©")
except OSError:
    print("T√©l√©chargement du mod√®le fran√ßais...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "spacy", "download", "fr_core_news_sm"])
    nlp = spacy.load("fr_core_news_sm")

# Configuration
random.seed(42)

# Cr√©er la structure de dossiers
os.makedirs('synthetic_languages/shuffle', exist_ok=True)
os.makedirs('synthetic_languages/reverse', exist_ok=True)
os.makedirs('synthetic_languages/hop', exist_ok=True)

# ============================================================================
# SECTION 2: EXTRACTION DES DONN√âES DEPUIS FQUAD
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 1: EXTRACTION DES DONN√âES")
print("=" * 80)

# Charger le dataset FQuAD
print("\nüìö Chargement du dataset FQuAD...")
ds = load_dataset("manu/fquad2_test")

# Extraire tous les contextes uniques
contexts = set()
for split_name in ds.keys():
    print(f"  - Traitement du split '{split_name}': {len(ds[split_name])} exemples")
    for item in ds[split_name]:
        if 'context' in item:
            contexts.add(item['context'])

contexts_list = list(contexts)
print(f"\n‚úì {len(contexts_list)} contextes uniques extraits")

# ============================================================================
# SECTION 3: DIVISION DES CONTEXTES EN PHRASES
# ============================================================================

def split_into_sentences(text: str) -> List[str]:
    """
    Divise un texte en phrases en g√©rant les abr√©viations fran√ßaises
    Filtre les phrases entre 10 et 50 mots
    """
    # G√©rer les abr√©viations courantes pour √©viter les fausses coupures
    abbreviations = {
        'M.': 'M@', 'Mme.': 'Mme@', 'Dr.': 'Dr@', 'Prof.': 'Prof@',
        'etc.': 'etc@', 'cf.': 'cf@', 'p.': 'p@', 'ex.': 'ex@',
        'c.-√†-d.': 'c@-√†-d@', 'av.': 'av@', 'apr.': 'apr@', 'J.-C.': 'J@-C@',
    }
    
    # Remplacer temporairement les abr√©viations
    for abbr, replacement in abbreviations.items():
        text = text.replace(abbr, replacement)
    
    # Diviser par les ponctuations de fin de phrase
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z√Ä-≈∏])', text)
    
    # Restaurer les abr√©viations
    for abbr, replacement in abbreviations.items():
        sentences = [s.replace(replacement, abbr) for s in sentences]
    
    # Nettoyer et filtrer par longueur
    sentences = [s.strip() for s in sentences if s.strip()]
    sentences = [s for s in sentences if 10 <= len(s.split()) <= 50]
    
    return sentences

print("\nüìù Division des contextes en phrases...")
all_sentences = []
for i, context in enumerate(contexts_list):
    if i % 100 == 0:
        print(f"  Progression: {i}/{len(contexts_list)}")
    sentences = split_into_sentences(context)
    all_sentences.extend(sentences)

# D√©dupliquer les phrases
unique_sentences = list(set(all_sentences))
print(f"\n‚úì {len(unique_sentences)} phrases uniques extraites")

# Cr√©er les splits train/validation/test
random.shuffle(unique_sentences)
total = len(unique_sentences)
train_size = int(0.8 * total)  # 80%
val_size = int(0.1 * total)    # 10%

train_sentences = unique_sentences[:train_size]
val_sentences = unique_sentences[train_size:train_size + val_size]
test_sentences = unique_sentences[train_size + val_size:]

print(f"\nüìä Splits cr√©√©s:")
print(f"  - Train: {len(train_sentences)} phrases")
print(f"  - Validation: {len(val_sentences)} phrases")
print(f"  - Test: {len(test_sentences)} phrases")

# ============================================================================
# SECTION 4: ANNOTATION POS AVEC SPACY
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 2: ANNOTATION POS")
print("=" * 80)

def annotate_sentence(sentence: str, nlp) -> Dict:
    """
    Annote une phrase avec les informations POS (Part-of-Speech)
    N√©cessaire pour les transformations *Hop
    """
    doc = nlp(sentence)
    
    tokens = []
    for token in doc:
        token_info = {
            "text": token.text,
            "lemma": token.lemma_,
            "pos": token.pos_,        # POS universel
            "tag": token.tag_,        # POS d√©taill√©
            "dep": token.dep_,        # D√©pendance syntaxique
            "morph": str(token.morph), # Traits morphologiques
            "is_alpha": token.is_alpha,
            "is_stop": token.is_stop,
            "is_punct": token.is_punct
        }
        
        # Informations sp√©cifiques pour les verbes (pour *Hop)
        if token.pos_ == "VERB":
            morph_dict = token.morph.to_dict()
            token_info["verb_form"] = morph_dict.get("VerbForm", "")
            token_info["tense"] = morph_dict.get("Tense", "")
            token_info["person"] = morph_dict.get("Person", "")
            token_info["number"] = morph_dict.get("Number", "")
        
        tokens.append(token_info)
    
    return {
        "original": sentence,
        "tokens": tokens,
        "num_tokens": len(tokens),
        "num_words": len([t for t in tokens if t["is_alpha"]]),
        "has_verb": any(t["pos"] == "VERB" for t in tokens)
    }

print("\nüè∑Ô∏è  Annotation des phrases avec spaCy...")
tagged_data = {
    "metadata": {
        "source": "FQuAD contexts",
        "total_unique_sentences": len(unique_sentences),
        "language": "fr",
        "tagging_model": "fr_core_news_sm",
        "description": "Phrases annot√©es pour l'exp√©rience Mission: Impossible Language Models"
    },
    "train": [],
    "validation": [],
    "test": []
}

# Annoter le train set
print("\n  Annotation du train set...")
for i, sentence in enumerate(train_sentences):
    if i % 200 == 0:
        print(f"    Progression: {i}/{len(train_sentences)}")
    tagged_data["train"].append(annotate_sentence(sentence, nlp))

# Annoter le validation set
print("\n  Annotation du validation set...")
for i, sentence in enumerate(val_sentences):
    if i % 50 == 0:
        print(f"    Progression: {i}/{len(val_sentences)}")
    tagged_data["validation"].append(annotate_sentence(sentence, nlp))

# Annoter le test set
print("\n  Annotation du test set...")
for i, sentence in enumerate(test_sentences):
    if i % 50 == 0:
        print(f"    Progression: {i}/{len(test_sentences)}")
    tagged_data["test"].append(annotate_sentence(sentence, nlp))

# Sauvegarder les donn√©es annot√©es
with open('tagged_sentences.json', 'w', encoding='utf-8') as f:
    json.dump(tagged_data, f, ensure_ascii=False, indent=2)

print("\n‚úì Donn√©es annot√©es sauvegard√©es dans 'tagged_sentences.json'")

# Statistiques sur les POS
verb_count = sum(1 for sent in tagged_data["train"] if sent["has_verb"])
print(f"\nüìä Statistiques:")
print(f"  - Phrases avec verbes dans le train: {verb_count}/{len(tagged_data['train'])} ({verb_count/len(tagged_data['train'])*100:.1f}%)")

# ============================================================================
# SECTION 5: D√âFINITION DES TRANSFORMATIONS
# ============================================================================

print("\n" + "=" * 80)
print("√âTAPE 3: CR√âATION DES LANGUES SYNTH√âTIQUES")
print("=" * 80)

# --------------- Transformations *SHUFFLE ---------------

def no_shuffle(sentence_data: Dict) -> str:
    """NoShuffle: Retourne la phrase originale (contr√¥le)"""
    return sentence_data['original']

def nondeterministic_shuffle(sentence_data: Dict) -> str:
    """NondeterministicShuffle: M√©lange al√©atoire complet des mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    random.shuffle(tokens)
    return ' '.join(tokens)

def deterministic_shuffle(sentence_data: Dict) -> str:
    """DeterministicShuffle: M√©lange d√©terministe bas√© sur la longueur"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    # Utiliser la longueur comme seed pour un m√©lange reproductible
    rng = random.Random(len(tokens))
    shuffled = tokens.copy()
    rng.shuffle(shuffled)
    return ' '.join(shuffled)

def local_shuffle(sentence_data: Dict) -> str:
    """LocalShuffle: M√©lange par petits blocs de 2-3 mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    result = []
    i = 0
    
    while i < len(tokens):
        # Taille du bloc al√©atoire (2 ou 3 mots)
        block_size = random.choice([2, 3])
        block = tokens[i:i+block_size]
        # M√©langer le bloc
        random.shuffle(block)
        result.extend(block)
        i += block_size
    
    return ' '.join(result)

def even_odd_shuffle(sentence_data: Dict) -> str:
    """EvenOddShuffle: S√©parer positions paires/impaires puis concat√©ner"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    even_tokens = [tokens[i] for i in range(0, len(tokens), 2)]
    odd_tokens = [tokens[i] for i in range(1, len(tokens), 2)]
    return ' '.join(even_tokens + odd_tokens)

# --------------- Transformations *REVERSE ---------------

def no_reverse(sentence_data: Dict) -> str:
    """NoReverse: Retourne la phrase originale (contr√¥le)"""
    return sentence_data['original']

def partial_reverse(sentence_data: Dict) -> str:
    """PartialReverse: Inverser une partie apr√®s un marqueur R plac√© al√©atoirement"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    
    if len(tokens) < 3:
        return ' '.join(tokens)
    
    # Placer le marqueur R √† une position al√©atoire
    r_position = random.randint(1, len(tokens) - 2)
    
    # Construire: d√©but + R + fin invers√©e
    result = tokens[:r_position] + ['R'] + tokens[r_position:][::-1]
    return ' '.join(result)

def full_reverse(sentence_data: Dict) -> str:
    """FullReverse: Inverser compl√®tement l'ordre des mots"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    return ' '.join(tokens[::-1])

# --------------- Transformations *HOP ---------------

def get_verb_indices(sentence_data: Dict) -> List[int]:
    """Trouve tous les indices des verbes dans la phrase"""
    return [i for i, token in enumerate(sentence_data['tokens']) 
            if token['pos'] == 'VERB']

def inflect_verb(token: Dict, should_pluralize: bool) -> str:
    """
    Inflexion simple du verbe fran√ßais
    Note: Impl√©mentation basique, une vraie conjugaison n√©cessiterait plus de r√®gles
    """
    text = token['text']
    
    # Si on doit mettre au pluriel un verbe singulier
    if should_pluralize and token.get('number') == 'Sing':
        if text.endswith('e'):
            return text + 'nt'
        elif text.endswith('t'):
            return text[:-1] + 'ent'
        else:
            return text + 'ent'
    # Si on doit mettre au singulier un verbe pluriel
    elif not should_pluralize and token.get('number') == 'Plur':
        if text.endswith('ent'):
            return text[:-3]
        elif text.endswith('nt'):
            return text[:-2]
    
    return text

def no_hop(sentence_data: Dict) -> str:
    """NoHop: Ajouter des marqueurs S/P mais sans transformation du verbe"""
    tokens = [t['text'] for t in sentence_data['tokens']]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    # Choisir un verbe al√©atoire
    verb_idx = random.choice(verb_indices)
    
    # Placer un marqueur S (singulier) ou P (pluriel)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(1, min(4, verb_idx)) if verb_idx > 0 else 0)
    
    result = tokens[:marker_position] + [marker] + tokens[marker_position:]
    return ' '.join(result)

def token_hop(sentence_data: Dict) -> str:
    """TokenHop: Inflexion du verbe bas√©e sur le comptage de tokens apr√®s le marqueur"""
    tokens_data = sentence_data['tokens']
    tokens = [t['text'] for t in tokens_data]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    verb_idx = random.choice(verb_indices)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(5, min(8, verb_idx)) if verb_idx > 5 else 0)
    
    # Compter 4 tokens apr√®s le marqueur
    count_position = marker_position + 4
    
    # R√®gle impossible: inflexion bas√©e sur la parit√© de la position
    should_pluralize = (count_position % 2 == 0) if marker == 'S' else (count_position % 2 == 1)
    
    # Construire la phrase avec verbe modifi√©
    result = tokens[:marker_position] + [marker] + tokens[marker_position:verb_idx]
    result.append(inflect_verb(tokens_data[verb_idx], should_pluralize))
    result.extend(tokens[verb_idx + 1:])
    
    return ' '.join(result)

def word_hop(sentence_data: Dict) -> str:
    """WordHop: Inflexion du verbe bas√©e sur le comptage de mots (non-ponctuation)"""
    tokens_data = sentence_data['tokens']
    tokens = [t['text'] for t in tokens_data]
    verb_indices = get_verb_indices(sentence_data)
    
    if not verb_indices:
        return ' '.join(tokens)
    
    verb_idx = random.choice(verb_indices)
    marker = random.choice(['S', 'P'])
    marker_position = max(0, verb_idx - random.randint(5, min(8, verb_idx)) if verb_idx > 5 else 0)
    
    # Compter 4 mots (pas la ponctuation) apr√®s le marqueur
    word_count = 0
    for i in range(marker_position, len(tokens_data)):
        if tokens_data[i]['is_alpha']:
            word_count += 1
            if word_count == 4:
                break
    
    # R√®gle impossible: inflexion bas√©e sur si on a atteint 4 mots
    should_pluralize = (word_count == 4) if marker == 'S' else (word_count != 4)
    
    # Construire la phrase
    result = tokens[:marker_position] + [marker] + tokens[marker_position:verb_idx]
    result.append(inflect_verb(tokens_data[verb_idx], should_pluralize))
    result.extend(tokens[verb_idx + 1:])
    
    return ' '.join(result)

# ============================================================================
# SECTION 6: APPLICATION DES TRANSFORMATIONS
# ============================================================================

# Dictionnaire de toutes les transformations
transformations = {
    # *Shuffle (5 variantes)
    'NoShuffle': no_shuffle,
    'NondeterministicShuffle': nondeterministic_shuffle,
    'DeterministicShuffle': deterministic_shuffle,
    'LocalShuffle': local_shuffle,
    'EvenOddShuffle': even_odd_shuffle,
    # *Reverse (3 variantes)
    'NoReverse': no_reverse,
    'PartialReverse': partial_reverse,
    'FullReverse': full_reverse,
    # *Hop (3 variantes)
    'NoHop': no_hop,
    'TokenHop': token_hop,
    'WordHop': word_hop
}

print("\nüîÑ Application des 11 transformations...")

# Appliquer chaque transformation
for trans_name, trans_func in transformations.items():
    print(f"\n  Transformation: {trans_name}")
    
    # Cr√©er le dataset transform√©
    transformed_data = {
        "metadata": {
            **tagged_data["metadata"],
            "transformation": trans_name,
            "transformation_type": "shuffle" if "Shuffle" in trans_name else "reverse" if "Reverse" in trans_name else "hop"
        }
    }
    
    # Transformer chaque split
    for split in ["train", "validation", "test"]:
        transformed_data[split] = []
        
        # R√©initialiser la seed pour la reproductibilit√©
        random.seed(42)
        
        errors = 0
        for sentence_data in tagged_data[split]:
            try:
                transformed_text = trans_func(sentence_data)
                transformed_data[split].append({
                    "original": sentence_data["original"],
                    "transformed": transformed_text,
                    "num_tokens": sentence_data["num_tokens"],
                    "has_verb": sentence_data["has_verb"]
                })
            except Exception as e:
                # En cas d'erreur, utiliser l'original
                errors += 1
                transformed_data[split].append({
                    "original": sentence_data["original"],
                    "transformed": sentence_data["original"],
                    "num_tokens": sentence_data["num_tokens"],
                    "has_verb": sentence_data["has_verb"]
                })
        
        if errors > 0:
            print(f"    ‚ö†Ô∏è  {errors} erreurs dans le split {split}")
    
    # D√©terminer le sous-dossier
    if trans_name.endswith("Shuffle"):
        subdir = "shuffle"
    elif trans_name.endswith("Reverse"):
        subdir = "reverse"
    else:
        subdir = "hop"
    
    # Sauvegarder
    output_path = f"synthetic_languages/{subdir}/{trans_name}.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(transformed_data, f, ensure_ascii=False, indent=2)
    
    print(f"    ‚úì Sauvegard√© dans {output_path}")
    
    # Afficher un exemple
    example = transformed_data["train"][0]
    print(f"    Exemple: {example['original'][:50]}...")
    print(f"    ‚Üí {example['transformed'][:50]}...")

# ============================================================================
# SECTION 7: R√âSUM√â FINAL
# ============================================================================

print("\n" + "=" * 80)
print("R√âSUM√â")
print("=" * 80)

print("\n‚úÖ Pr√©paration des donn√©es termin√©e avec succ√®s!")
print(f"\nüìä Statistiques finales:")
print(f"  - Contextes FQuAD trait√©s: {len(contexts_list)}")
print(f"  - Phrases uniques extraites: {len(unique_sentences)}")
print(f"  - R√©partition: {len(train_sentences)} train / {len(val_sentences)} val / {len(test_sentences)} test")
print(f"  - Transformations appliqu√©es: 11")

print("\nüìÅ Fichiers cr√©√©s:")
print("  - tagged_sentences.json (donn√©es annot√©es)")
print("  - synthetic_languages/")
print("    ‚îú‚îÄ‚îÄ shuffle/ (5 fichiers)")
print("    ‚îú‚îÄ‚îÄ reverse/ (3 fichiers)")
print("    ‚îî‚îÄ‚îÄ hop/ (3 fichiers)")

print("\nüéØ Prochaine √©tape:")
print("  Utiliser ces donn√©es pour entra√Æner des mod√®les de langage")
print("  et comparer les performances entre langues naturelles et impossibles")

print("\n" + "=" * 80)
print("FIN DU SCRIPT")
print("=" * 80)